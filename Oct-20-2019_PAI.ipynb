{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: 06/19/2019\n",
    "\n",
    "## Brief: What was done previously\n",
    "Previous work discovered a new architecture that approximated a PDE\n",
    "\n",
    "## Hypothesis\n",
    "1. Including local values in feature embedding improves prediction\n",
    "1. Adding valid bit-mask to kernels reduces edge effects\n",
    "1. Using richer feature space reduces edge effects\n",
    "\n",
    "\n",
    "## Summary of Main Results and Discussions\n",
    "Modeled new architecture that uses skip layers to approximate PDE\n",
    "\n",
    "New architecture achieves better performance fig ? with fewer model parameters. Additionally, this formulation is \n",
    "more natural for learned functions allowing approximation by traditional PDE equations after learning a deep model.\n",
    "\n",
    "\n",
    "### Hypothesis 1 results and discussion\n",
    "Leaned models handle noise suppression impressively well, dealing with noise with standard deviation twice as large as the range of the sample data. \n",
    "\n",
    "### Experiment 2: results and discussion\n",
    "Put main result and conclusions here. Discuss importance/impact in terms of the project goals.\n",
    "\n",
    "\n",
    "## Plan for next effort\n",
    "Test architectures that remove the edge effect / dont penalize edge effects for patches\n",
    "Perform hyper parameter optimization\n",
    "Test training at multiple scales (LM-architecture)\n",
    "Test the temporal consistency of networks\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hypothesis 1\n",
    "### Including local values in feature encoding improves prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the effect of using 1x1, 3x3, and 5x5 kernels for feature encoding\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import src.predict_pde_recurrent\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "completed = [] #itertools.product([5, 3, 1], [20, 10, 5])\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "for encoding_kernel_size, pred_len in itertools.product([5, 3, 1], [20, 10, 5]):\n",
    "    if (encoding_kernel_size, pred_len) not in completed:\n",
    "        name = 'enc_kernel_{}_test-conv_3-skip_1-cell'.format(encoding_kernel_size)\n",
    "        num_batches = 15000 + pred_len * 1000        \n",
    "        model = src.predict_pde_recurrent.train(net_name=name, \n",
    "                                                encoder_kernel_size=encoding_kernel_size,\n",
    "                                                pred_length=pred_len, \n",
    "                                                history_length=5,\n",
    "                                                num_batches=num_batches)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accruacy of different runs\n",
    "import os\n",
    "import glob\n",
    "# import itertools\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "completed = list(itertools.product([5, 3, 1], [20, 10, 5]))\n",
    "\n",
    "\n",
    "# exp_root = 'F:\\\\'\n",
    "exp_root = 'C:\\\\Users\\\\brandon\\\\source\\\\orbitalMechanics\\\\experiments\\\\turbulence\\\\pde\\\\'\n",
    "exp_folders = [exp_root + 'enc_kernel_{}_test-conv_3-skip_1-cell_0.0005-lr_5-hist_{}-pred'.format(k, l) for (k,l) in completed]\n",
    "# exp_folders = glob.glob(exp_root + '/enc_kernel_[0-9]*_test-conv_3-skip_1-cell*30-pred')\n",
    "npys = ['train_accuracy_by_time.npz', 'validation_accuracy_by_time.npz']\n",
    "\n",
    "train_data = []\n",
    "validation_data = []\n",
    "diff = []\n",
    "print(len(exp_folders),len(completed))\n",
    "\n",
    "for i, (directory, (k,l)) in enumerate(zip(exp_folders, completed)):\n",
    "    # Load input, prediction and label\n",
    "    try:\n",
    "        if all([os.path.exists(os.path.join(exp_root, directory, f)) for f in npys]):\n",
    "            ts = '10000'\n",
    "            for key in set(train_acc.keys()).intersection(valid_acc.keys()):\n",
    "                if int(key) > int(ts) or ts is None:\n",
    "                    ts = key\n",
    "            print(ts)\n",
    "            train_acc_dir, valid_acc_dir = [os.path.join(exp_root, directory, f) for f in npys]\n",
    "\n",
    "            with np.load(train_acc_dir) as train_acc, np.load(valid_acc_dir) as valid_acc:\n",
    "                train_data.append(\n",
    "                    go.Scatter(\n",
    "                        x = list(range(train_acc[ts].shape[0])),\n",
    "                        y = train_acc[ts], \n",
    "                        name = 'kernel_size={} len={}'.format(k, l)))\n",
    "                validation_data.append(\n",
    "                    go.Scatter(\n",
    "                        x = list(range(valid_acc[ts].shape[0])),\n",
    "                        y = valid_acc[ts], \n",
    "                        name = 'kernel_size={} len={}'.format(k, l)))\n",
    "                diff.append(\n",
    "                    go.Scatter(\n",
    "                        x = list(range(valid_acc[ts].shape[0])),\n",
    "                        y = valid_acc[ts] - train_acc[ts], \n",
    "                        name = 'kernel_size={} len={}'.format(k, l)))\n",
    "        else:\n",
    "            print('skipped', directory)\n",
    "    except KeyError:\n",
    "        continue\n",
    "            \n",
    "layout = go.Layout(\n",
    "    title=\"Prediction Accuracy by Time Horizon - Train\",\n",
    "    xaxis=dict(\n",
    "        type='linear',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Predicted time step, t',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        tickmode = 'array',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Mean squared validation error n = 20',\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig = go.Figure(data=train_data, layout=layout)\n",
    "py.iplot(fig, filename='acc_over_time_noise_study_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 2\n",
    "### Reduce scope of local values for pde estimation (lower cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the effect of using 1x1, 3x3, and 5x5 kernels for pde layer encoding\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import src.predict_pde_recurrent\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "completed = [] #itertools.product([5, 3, 1], [20, 10, 5])\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "for kernel_size, pred_len in itertools.product([5, 3, 1], [20, 10, 5]):\n",
    "    if (kernel_size, pred_len) not in completed:\n",
    "        name = 'kernel_size_{}-conv_3-skip_1-cell'.format(kernel_size)\n",
    "        num_batches = 15000 + pred_len * 1000        \n",
    "        model = src.predict_pde_recurrent.train(net_name=name, \n",
    "                                                conv_width=kernel_size,\n",
    "                                                pred_length=pred_len, \n",
    "                                                history_length=5,\n",
    "                                                num_batches=num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem formulation\n",
    "Let $x$ represent h patches of history $x = [x_{t-h}, ..., x_{t-1}]$ and y represent the target sequence $y = \n",
    "[x_{t}, x_{t+1},...,x_{t+l-1}]$.  We learn an encoder, $\\phi(x) = u_0$, a decoder $\\theta(u_n) = \\hat{y}_n$ and a \n",
    "dynamics model $f(u_n)$ from equation \\ref{eqn:ode}    \n",
    "Rather \n",
    "than \n",
    "explicitly parameterizing the ODE on derivatives of $x$, we instead consider a system of ordinary differential\n",
    " equations of dimension m = 16. We then learn the encoding from 5 frames of history \n",
    " encoding local features via\n",
    " \n",
    " Thus we learn a series of features $w_{enc}$ such that $u_0 = relu(x * w_{enc})$.\n",
    " We approximate the dynamics $f(u)$ by    $f(u_n)$ by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import src.predict_pde_recurrent\n",
    "completed = [(5,5), (5,20), (5,40), (5,60)]\n",
    "\n",
    "for (history, pred_len) in itertools.product([5, 20], [5, 20, 40, 60]):\n",
    "    if (history, pred_len) not in completed:\n",
    "        name = 'conv_3-skip_1-cell'\n",
    "        num_batches = 15000 + pred_len * 1000\n",
    "        model = src.predict_pde_recurrent.train(net_name=name, pred_length=pred_len, history_length=history, \n",
    "                                                num_batches=num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import packages \n",
    "import importlib\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 1\n",
    "\n",
    "Deep recurrent networks are tolerant to sensor noise below a certain magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure accuracy over increasing fixed gaussian sensor noise\n",
    "We add increasing magnitudes of gaussian noise to the input and predict the clean original signal. We expect, for\n",
    "high levels of noise, the model to over-fit to the noise. However, after some threshold we expect the model to learn to recover from small perturbations by learning the underlying distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Gaussian noise study (fixed input noise)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.predict_pde_recurrent import train\n",
    "from src.dataLoader.turbulence import Turbulence, RANDOM_SEED, LARGE_DATASET\n",
    "\n",
    "# Use a fixed seed for noise    \n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "for scale in [0, 0.01, 0.05, 0.1, 0.25]:\n",
    "    noise_data = np.random.normal(size=(360, 279, 1000), scale=scale)\n",
    "    \n",
    "    loader = Turbulence(pred_length=40, dataset_idx=LARGE_DATASET, input_noise=noise_data, debug=False)\n",
    "    \n",
    "    train(loader=loader, dataset_idx=LARGE_DATASET, num_batches=50000, net_name='PDE_3-skip_1-cell_resnet_static_noise_{}'.format(scale))\n",
    "    \n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore results - Hypothesis 1\n",
    "\n",
    "In this test given a 3 layer encoder/decoder model with 250 units per layer, we see that the performance of the model is \n",
    "resistant to up to 5% noise without any degradation. Further noise causes significant increases in L2 loss porportinal to the magnitude of of noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare accuracy of model with increasing fixed noise\n",
    "import os\n",
    "import plotly.plotly as py\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import plotly\n",
    "\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "plt.rc('text', usetex=True)\n",
    "    \n",
    "# Compare MSE vs magnitude of noise\n",
    "noise = [2, 1, 0.75, 0.5, 0.25, 0.1, 0.05, 0.025, 0.01, 0.005, 0.0025, 0.0001, 0]\n",
    "noise.reverse()\n",
    "train_accuracy = [1.836e-5, 1.8441e-5, 2.0871e-5, 2.0525e-5, 2.0918e-5, 2.2431e-5, 2.2850e-5, 3.6688e-5, 6.5500e-5, 7.1376e-5, 7.3004e-5, 7.1560e-5, 1.1372e-4]\n",
    "validation_accuracy = [1.932e-5, 1.9536e-5, 2.1711e-5, 2.1794e-5, 2.1213e-5, 2.3923e-5, 2.4532e-5, 4.2984e-5, 1.0194e-4, 2.9481e-4, 5.4323e-4, 7.5883e-4, 2.0583e-3]\n",
    "\n",
    "# Create a trace\n",
    "trace1 = go.Scatter(\n",
    "    x = noise,\n",
    "    y = validation_accuracy,\n",
    "    name=\"validation\"\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x = noise,\n",
    "    y = train_accuracy,\n",
    "    name=\"train\"\n",
    ")\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    title=\"Magnitude of Sensor Noise vs L2 Loss\",\n",
    "    xaxis=dict(\n",
    "        type='log',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Standard Deviation of Added Gaussian Noise',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='log',\n",
    "        tickmode = 'array',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Mean Squared Validation Error n=20',\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='static_noise_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title=\"Magnitude of Sensor Noise vs L2 Loss\",\n",
    "    xaxis=dict(\n",
    "        type='log',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Standard Deviation of Added Gaussian Noise',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='log',\n",
    "        tickmode = 'array',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Mean Squared Error n = 20',\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='static_noise_model_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy over time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# exp_root = 'F:\\\\'\n",
    "exp_root = 'C:\\\\Users\\\\brandon\\\\source\\\\orbitalMechanics\\\\experiments\\\\turbulence\\\\pde'\n",
    "exp_folders = ['conv_3-skip_1-cell_velocity_and_vorticity_field_1200s.mat_0.0005-lr_h-hist_l-pred'.format(h,l) \n",
    "                for h,l in completed]\n",
    "npys = ['train_accuracy_by_time.npz', 'validation_accuracy_by_time.npz']\n",
    "\n",
    "train_data = []\n",
    "validation_data = []\n",
    "diff = []\n",
    "\n",
    "for i, directory in enumerate(exp_folders):\n",
    "    # Load input, prediction and label\n",
    "    try:\n",
    "        if all([os.path.exists(os.path.join(exp_root, directory, f)) for f in npys]):\n",
    "            ts = '30000'\n",
    "            train_acc_dir, valid_acc_dir = [os.path.join(exp_root, directory, f) for f in npys]\n",
    "\n",
    "            with np.load(train_acc_dir) as train_acc, np.load(valid_acc_dir) as valid_acc:\n",
    "                train_data.append(\n",
    "                    go.Scatter(\n",
    "                        x = list(range(train_acc[ts].shape[0])),\n",
    "                        y = train_acc[ts], \n",
    "                        name = 'sigma^2={}'.format(noise[i])))\n",
    "                validation_data.append(\n",
    "                    go.Scatter(\n",
    "                        x = list(range(valid_acc[ts].shape[0])),\n",
    "                        y = valid_acc[ts], \n",
    "                        name = 'sigma^2={}'.format(noise[i])))\n",
    "                diff.append(\n",
    "                    go.Scatter(\n",
    "                        x = list(range(valid_acc[ts].shape[0])),\n",
    "                        y = valid_acc[ts] - train_acc[ts], \n",
    "                        name = 'sigma^2={}'.format(noise[i])))\n",
    "        else:\n",
    "            print('skipped', directory)\n",
    "    except KeyError:\n",
    "        continue\n",
    "            \n",
    "layout = go.Layout(\n",
    "    title=\"Prediction Accuracy by Time Horizon - Train\",\n",
    "    xaxis=dict(\n",
    "        type='linear',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Predicted time step, t',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        tickmode = 'array',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Mean squared validation error n = 20',\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig = go.Figure(data=train_data, layout=layout)\n",
    "py.iplot(fig, filename='acc_over_time_noise_study_train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "    title=\"Prediction Accuracy by Time Horizon - Validation\",\n",
    "    xaxis=dict(\n",
    "        type='linear',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Predicted time step, t',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        tickmode = 'array',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Mean squared validation error n = 20',\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig = go.Figure(data=validation_data, layout=layout)\n",
    "py.iplot(fig, filename='acc_over_time_noise_study_validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "    title=\"Prediction Accuracy by Time Horizon - Difference\",\n",
    "    xaxis=dict(\n",
    "        type='linear',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Predicted time step, t',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        tickmode = 'array',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Diffrence in MSE between train and validation; n = 20',\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig = go.Figure(data=diff, layout=layout)\n",
    "py.iplot(fig, filename='acc_over_time_noise_study_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small magnitude of fixed noise increases generalization to new samples\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "exp_root = 'F:\\\\'\n",
    "exp_folders = ['lstm_5_cells_dropout_60_static_noise_{}_velocity_and_vorticity_field_1200s.mat_lr0.0001'.format(n) for n in noise]\n",
    "npys = ['train_accuracy_by_time.npz', 'validation_accuracy_by_time.npz']\n",
    "\n",
    "train = []\n",
    "validation = []\n",
    "x = []\n",
    "\n",
    "\n",
    "for i, directory in enumerate(exp_folders):\n",
    "    # Load input, prediction and label\n",
    "    if all([os.path.exists(os.path.join(exp_root, directory, f)) for f in npys]):\n",
    "        ts = '50000'\n",
    "        train_acc_dir, valid_acc_dir = [os.path.join(exp_root, directory, f) for f in npys]\n",
    "\n",
    "        with np.load(train_acc_dir) as train_acc, np.load(valid_acc_dir) as valid_acc:\n",
    "            try:\n",
    "                train.append(train_acc[ts][19]) \n",
    "                validation.append(valid_acc[ts][19])\n",
    "                x.append(noise[i] + 0.0001)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "train_data = go.Scatter(\n",
    "        x = x,\n",
    "        y = train, \n",
    "        name = 'train',\n",
    "        text=[str(p - 0.0001) for p in x])\n",
    "validation_data = go.Scatter(\n",
    "        x = x,\n",
    "        y = validation, \n",
    "        name = 'validation',\n",
    "        text=[str(p - 0.0001) for p in x])\n",
    "            \n",
    "layout = go.Layout(\n",
    "    title=\"Sensor noise vs prediction error at step t+20\",\n",
    "    xaxis=dict(\n",
    "        type='linear',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Standard deviation of added fixed gaussian noise',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        tickmode = 'array',\n",
    "        autorange=True,\n",
    "        showexponent = 'all',\n",
    "        exponentformat = 'e',\n",
    "        title='Mean squared validation error at step t+20',\n",
    "    )\n",
    ")\n",
    "data=[validation_data]\n",
    "    \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='acc_over_time_noise_study_train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize noise data\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Make sub-plots for input sequence\n",
    "plt.rc('text', usetex=True)\n",
    "fig, plots = plt.subplots(5, 8, figsize=(24, 17))\n",
    "\n",
    "exp_root = 'F:\\\\'\n",
    "exp_folders = ['lstm_5_cells_dropout_60_static_noise_{}_velocity_and_vorticity_field_1200s.mat_lr0.0001'.format(n) for n in noise]\n",
    "titles = ['Sequence + N $\\sig = {} \\mu = 0$'.format(n) for n in noise if n != 0.0001]\n",
    "titles = [r\"$X + \\mathcal{N}(\\mu,\\,\\sigma^{2}=\" + str(n) + \") $\" for n in noise if n != 0.0001]\n",
    "\n",
    "for j in range(5):\n",
    "    for i, directory in enumerate(exp_folders[:8]):\n",
    "        for file in os.listdir(os.path.join(exp_root, directory)):\n",
    "            if file.endswith('.npz') and file.startswith('inputs'):\n",
    "                file_path = os.path.join(exp_root, directory, file)\n",
    "                with np.load(file_path) as foo:\n",
    "                    try:\n",
    "                        plots[j, i].imshow(np.reshape(foo['100000'][j*4, 0,:], (50,50)))\n",
    "                        plots[j, i].set_title(titles[i], fontsize=16, color='gray')\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "    \n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "                \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make videos of label, prediction, and difference\n",
    "import os\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Video parameters\n",
    "fps = 10\n",
    "height, width = 50, 50\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "\n",
    "# Convert [0, 1] np.float32 to [0, 255] RGB cv2.uint8\n",
    "cmap = plt.get_cmap('jet')\n",
    "def convert_img(img):\n",
    "    mapped = cmap(img)\n",
    "    return cv2.normalize(mapped[:,:,:3], None, 255, 0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "def add_lables(img):\n",
    "    font = cv2.FONT_HERSHEY_TRIPLEX \n",
    "    cv2.putText(img,'X',(10,40), font, 0.4,(0,0,0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(img,'f(X)',(10,90), font, 0.4,(0,0,0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(img,'Y',(10,140), font, 0.4,(0,0,0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(img,'|f(x) - y|^2', (10,190), font, 0.4,(0,0,0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(img,'X',(10,40), font, 0.4,(255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(img,'f(X)',(10,90), font, 0.4,(255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(img,'Y',(10,140), font, 0.4,(255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(img,'|f(x) - y|^2', (10,190), font, 0.4,(255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "# initialize video writer\n",
    "video_filename = 'output.avi'\n",
    "\n",
    "# noise = [2, 1, 0.75, 0.5, 0.25, 0.1, 0.05, 0.025, 0.01, 0.005, 0.0025, 0.0001, 0]\n",
    "\n",
    "\n",
    "exp_root = 'C:\\\\Users\\\\brandon\\\\source\\\\orbitalMechanics\\\\experiments\\\\turbulence\\\\pde'\n",
    "exp_folders = ['kernel_size_{}-conv_3-skip_1-cell_0.0005-lr_5-hist_{}-pred'.format(h,l) \n",
    "                for h,l in itertools.product([5, 3, 1], [20, 10, 5])]\n",
    "titles = ['kernel_size_{}, len={}'.format(h,l) for h,l in itertools.product([5, 3, 1], [20, 10, 5])]\n",
    "npys = ['inputs.npz', 'predictions.npz', 'labels.npz']\n",
    "\n",
    "for i, directory in enumerate(exp_folders):\n",
    "    # Load input, prediction and label\n",
    "    if all([os.path.exists(os.path.join(exp_root, directory, f)) for f in npys]):\n",
    "        print('rendering video for', directory)\n",
    "        input_dir, pred_dir, label_dir = [os.path.join(exp_root, directory, f) for f in npys]\n",
    "\n",
    "        with np.load(input_dir) as inputs, np.load(pred_dir) as predictions, np.load(label_dir) as labels:\n",
    "            ts = '10000'\n",
    "            for key in set(inputs.keys()).intersection(predictions.keys()):\n",
    "                if int(key) > int(ts) or ts is None:\n",
    "                    ts = key\n",
    "            print(ts)\n",
    "            out = cv2.VideoWriter(directory + '.avi', fourcc, fps, (width*16, height*4))\n",
    "\n",
    "            # First show input sequence\n",
    "            for i in range(inputs[ts].shape[0]):\n",
    "                top = np.concatenate(\n",
    "                        [np.reshape(inputs[ts][i, j,:], (50,50)) for j in range(16)], axis=1)\n",
    "                mid = np.concatenate(\n",
    "                        [np.zeros((50, 50), dtype='float32') for _ in range(16)], axis=1)\n",
    "                bot = np.concatenate(\n",
    "                        [np.zeros((50, 50), dtype='float32') for _ in range(16)], axis=1)\n",
    "                end = np.concatenate(\n",
    "                        [np.zeros((50, 50), dtype='float32') for _ in range(16)], axis=1)\n",
    "                         \n",
    "                img = convert_img(np.concatenate([top, mid, bot, end], axis=0))\n",
    "                add_lables(img)\n",
    "                print(img.shape)\n",
    "                out.write(img)\n",
    "                \n",
    "            # Then show prediction, label, and diffrence\n",
    "            for i in range(labels[ts].shape[0]):\n",
    "                top = np.concatenate(\n",
    "                        [np.reshape(inputs[ts][19, j,:], (50,50)) for j in range(16)], axis=1)\n",
    "                mid = np.concatenate(\n",
    "                        [np.reshape(predictions[ts][i, j,:], (50,50)) for j in range(16)], axis=1)\n",
    "                bot = np.concatenate(\n",
    "                        [np.reshape(labels[ts][i, j,:], (50,50)) for j in range(16)], axis=1)\n",
    "                end = np.concatenate(\n",
    "                        [2**(np.reshape(predictions[ts][i, j,:], (50,50)) - np.reshape(labels[ts][i, j,:], (50,50))) for j in range(16)], axis=1)\n",
    "                \n",
    "                img = convert_img(np.concatenate([top, mid, bot, end], axis=0))\n",
    "                add_lables(img)\n",
    "                print(img.shape)\n",
    "                out.write(img)\n",
    "\n",
    "            # Now hold the last frame for 20 frames\n",
    "            for _ in range(20):\n",
    "                out.write(img)\n",
    "            \n",
    "            out.release()\n",
    "            \n",
    "    else:\n",
    "        print('skipped', directory)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<h1>Hello, world!</h1>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis 2\n",
    "\n",
    "Deep recurrent networks are tolerant to missing samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Test 1\n",
    "\n",
    "Train modls with increasing magnitudes of random missing (zeroed) samples and compare their performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing samples study (random missing pixels)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.predict_turbulence_recurrent import train\n",
    "from src.dataLoader.turbulence import Turbulence, RANDOM_SEED, LARGE_DATASET\n",
    "\n",
    "# Use a fixed seed  \n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "for s in [0.9, 0.75, 0.5, 0.25, 0.1]:\n",
    "        \n",
    "    loader = Turbulence(pred_length=20, dataset_idx=LARGE_DATASET, debug=False)\n",
    "    \n",
    "    train(loader=loader, dataset_idx=LARGE_DATASET, num_batches=100000, pixel_dropout=s,\n",
    "          net_name='lstm_3_cells_20_pixel_dropout_{}'.format(s))\n",
    "    \n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "Comparing model perfomance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best lstm network\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.predict_turbulence_recurrent import train\n",
    "from src.dataLoader.turbulence import Turbulence, LARGE_DATASET\n",
    "\n",
    "loader = Turbulence(pred_length=20, dataset_idx=LARGE_DATASET, input_noise=noise_data, debug=False)\n",
    "\n",
    "train(loader=loader, dataset_idx=LARGE_DATASET, num_batches=100000, net_name='lstm_3_cells_+20_{}'.format(scale))\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make videos of label, prediction, and difference\n",
    "import cv2\n",
    "\n",
    "# Video parameters\n",
    "fps = 8\n",
    "height, width = 50, 50\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "\n",
    "# Convert [0, 1] np.float32 to [0, 255] RGB cv2.uint8\n",
    "cmap = plt.get_cmap('jet')\n",
    "def convert_img(img):\n",
    "    mapped = cmap(img)\n",
    "    return cv2.normalize(mapped[:,:,:3], None, 255, 0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "\n",
    "# initialize video writer\n",
    "video_filename = 'output.avi'\n",
    "\n",
    "exp_root = './experiments/turbulence/pde'\n",
    "exp_folders = ['conv']\n",
    "npys = ['inputs.npz', 'predictions.npz', 'labels.npz']\n",
    "\n",
    "for i, directory in enumerate(exp_folders):\n",
    "    # Load input, prediction and label\n",
    "    if all([os.path.exists(os.path.join(exp_root, directory, f)) for f in npys]):\n",
    "        print('done')\n",
    "        ts = '10000'\n",
    "        input_dir, pred_dir, label_dir = [os.path.join(exp_root, directory, f) for f in npys]\n",
    "\n",
    "        with np.load(input_dir) as inputs, np.load(pred_dir) as predictions, np.load(label_dir) as labels:\n",
    "\n",
    "            out = cv2.VideoWriter(video_filename, fourcc, fps, (width, height*4))\n",
    "\n",
    "            # First show input sequence\n",
    "            for i in range(inputs[ts].shape[0]):\n",
    "                top = convert_img(np.reshape(inputs[ts][i, 0,:], (50,50)))\n",
    "                mid = np.zeros((50, 50, 3), dtype='uint8')\n",
    "                bot = np.zeros((50, 50, 3), dtype='uint8')\n",
    "                dif = np.zeros((50, 50, 3), dtype='uint8')\n",
    "                print(np.concatenate([top, mid, bot, dif], axis=0).shape)\n",
    "                out.write(np.concatenate([top, mid, bot, dif], axis=0))\n",
    "                \n",
    "            # Then show prediction and label\n",
    "            for i in range(labels[ts].shape[0]):\n",
    "                top = convert_img(np.reshape(inputs[ts][19, 0,:], (50,50)))\n",
    "                mid = convert_img(np.reshape(predictions[ts][i, 0,:], (50,50)))\n",
    "                bot = convert_img(np.reshape(labels[ts][i, 0,:], (50,50)))\n",
    "                dif = convert_img(10*2**(np.reshape(predictions[ts][i, 0,:], (50,50)) - np.reshape(labels[ts][i, 0,:], (50,50))))\n",
    "                print(np.concatenate([top, mid, bot, dif], axis=0).shape)\n",
    "                out.write(np.concatenate([top, mid, bot, dif], axis=0))\n",
    "                \n",
    "            # Then hold last image\n",
    "            for i in range(10):\n",
    "                out.write(np.concatenate([top, mid, bot, dif], axis=0))\n",
    "\n",
    "            # Now hold the last frame and show label and prediction beneth\n",
    "            out.release()\n",
    "            break\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Baseline\n",
    "Below we use back-propigation to learn a linear model for predicting the next frame in the sequence. Back-propigation is  overkill for linear models, howerver the traing time is so fast it's not worth loading the problem into a least-squares minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import src.predict_pde_recurrent\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False \n",
    "# completed = []\n",
    "completed = [(5,5), (5,20)]\n",
    "\n",
    "def lin_residual_cell(X):\n",
    "    # Input 50 x 50 X history_len patch\n",
    "    return tf.layers.conv2d(\n",
    "        X, 1, 5, name='lin_res_layer', padding='same', activation=None)\n",
    "    \n",
    "    return head\n",
    "\n",
    "def pde(X, skip_depth=3, num_features=1, kernel_size=5, encoder_kernel_size=1):\n",
    "    # Input 50 x 50 X history_len patch\n",
    "    # Map this patch to match the residual cell size\n",
    "    head = tf.layers.conv2d(\n",
    "        X, filters=num_features, kernel_size=encoder_kernel_size, activation=None, name='map_input',\n",
    "        padding='valid')\n",
    "\n",
    "    head = tf.scan(\n",
    "        fn=lambda acc, _: lin_residual_cell(acc),\n",
    "        elems=tf.zeros(pred_len), initializer=head, swap_memory=True)\n",
    "\n",
    "    head = tf.map_fn(\n",
    "        fn=lambda elem:\n",
    "            tf.layers.conv2d(inputs=elem, filters=1, kernel_size=1, activation=None, name='map_output'),\n",
    "        elems=head\n",
    "    )\n",
    "\n",
    "    head = tf.squeeze(head)\n",
    "\n",
    "    return head\n",
    "\n",
    "for (history, pred_len) in itertools.product([5], [5, 20, 40, 60]):\n",
    "    if (history, pred_len) not in completed:\n",
    "        name = 'single_relu_pde'\n",
    "        num_batches = 15000 + pred_len * 1000\n",
    "        model = src.predict_pde_recurrent.train(\n",
    "            net_name=name, \n",
    "            pred_length=pred_len, \n",
    "            history_length=history, \n",
    "            network=pde,                                    \n",
    "            num_batches=num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runge Kutta method\n",
    "\n",
    "First try - just add the definition for RK4 to the network. We see that we learn more slowly (this could be bacause of\n",
    "the increasing constraints on the learned model) and that for all prediction lengths greater than 5 there was basically no learning at all. We would like to try an experiment where we add prediction steps durring training but this could be complicated to implement in the current (non-eager) execution environment (tensorflow)\n",
    "\n",
    "Because it was an issue with training - we quadruple the alloted training time and ajusted relu activations to be leaky to prevent gradients from disapearing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import src.predict_pde_recurrent\n",
    "import tensorflow as tf\n",
    "\n",
    "# from src.predict_pde_recurrent import residual_cell\n",
    "\n",
    "completed = []\n",
    "# completed = [(5,5), (5,20)]\n",
    "\n",
    "def residual_cell(activation, skip_depth, num_features, kernel_width):\n",
    "    num_filters = num_features\n",
    "    kernel_size = (kernel_width, kernel_width)\n",
    "\n",
    "    # Pass activation to last layer and cell layers\n",
    "    head = activation\n",
    "\n",
    "    # Cell layers\n",
    "    for i in range(skip_depth):\n",
    "        head = tf.layers.conv2d(\n",
    "            head, num_filters, kernel_size, name='res_layer_{}'.format(i),\n",
    "            padding='same')\n",
    "        if i == skip_depth - 1:\n",
    "            head += activation\n",
    "        head = tf.nn.sigmoid(head)\n",
    "\n",
    "    # Skip layer\n",
    "    return head\n",
    "\n",
    "def runge_kutta_pde(X, pred_len, is_training, skip_depth=5, num_features=8, kernel_size=5, encoder_kernel_size=1):\n",
    "    # Input 50 x 50 X history_len patch\n",
    "    # Map this patch to match the residual cell size\n",
    "    print(X.shape)\n",
    "    head = tf.layers.conv2d(\n",
    "        X, filters=num_features, kernel_size=encoder_kernel_size, activation=tf.nn.sigmoid, name='map_input',\n",
    "        padding='valid')\n",
    "    print(head.shape)\n",
    "    head = tf.layers.batch_normalization(head, training=is_training)\n",
    "    def rk4(y):\n",
    "        with tf.variable_scope('rk4', reuse=tf.AUTO_REUSE):\n",
    "            k1 = residual_cell(y, skip_depth, num_features, kernel_size)\n",
    "            k2 = residual_cell(y + 0.5 * k1, skip_depth, num_features, kernel_size)\n",
    "            k3 = residual_cell(y + 0.5 * k2, skip_depth, num_features, kernel_size)\n",
    "            k4 = residual_cell(y + k3, skip_depth, num_features, kernel_size)\n",
    "            return (k1 + k2 + k3 + k4) / 4\n",
    "\n",
    "    head = tf.scan(\n",
    "        fn=lambda acc, _: rk4(acc),\n",
    "        elems=tf.zeros(pred_len), initializer=head, swap_memory=True)\n",
    "    print(head.shape)\n",
    "    head = tf.map_fn(\n",
    "        fn=lambda elem:\n",
    "            tf.layers.conv2d(inputs=elem, filters=1, kernel_size=1, activation=tf.nn.sigmoid, name='map_output'),\n",
    "        elems=head\n",
    "    )\n",
    "    print(head.shape)\n",
    "\n",
    "    head = tf.squeeze(head)\n",
    "\n",
    "    return head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (history, pred_len) in itertools.product([5], [5, 10, 20, 40]):\n",
    "    if (history, pred_len) not in completed:\n",
    "        name = 'runge_kutta_leaky_relu'\n",
    "        num_batches = 15000 + pred_len * 4000\n",
    "        model = src.predict_pde_recurrent.train(\n",
    "            net_name=name, \n",
    "            pred_length=pred_len, \n",
    "            history_length=history, \n",
    "            network=runge_kutta_pde,                                    \n",
    "            num_batches=num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using increasing prediction length durring training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0920 12:47:47.254774  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:132: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0920 12:47:57.180961  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:151: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0920 12:47:57.180961  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:155: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0920 12:47:59.622300  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:160: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (?, 50, 50, 5)\n",
      "Output shape: (2, ?, 50, 50)\n",
      "(?, 50, 50, 5)\n",
      "(?, 50, 50, 8)\n",
      "(2, ?, 50, 50, 8)\n",
      "(2, ?, 50, 50, 1)\n",
      "Network shape: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0920 12:48:03.921832  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:256: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
      "\n",
      "W0920 12:48:03.921832  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:256: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "W0920 12:48:03.955431  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:270: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0920 12:48:03.956428  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:272: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0920 12:48:04.516808  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:281: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0920 12:48:04.518803  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:285: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0920 12:48:04.545548  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:294: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0920 12:48:04.545548  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:298: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0920 12:48:04.561140  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:329: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0920 12:48:05.941321  7804 deprecation_wrapper.py:119] From C:\\Users\\brandon\\source\\orbitalMechanics\\src\\predict_pde_recurrent.py:347: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462d10bedc8f48a2b5eac560b1683c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try to exclude deprecation warnings\n",
    "for pred_len in range(58):\n",
    "    num_batches = 6000\n",
    "    model = src.predict_pde_recurrent.train(\n",
    "        net_name = 'runge_kutta_growing_pred_len_sigmoid_6k_per_step_batch_norm_8_features_5_layers',\n",
    "        pred_length=pred_len + 2, \n",
    "        history_length=5, \n",
    "        network=runge_kutta_pde,                                    \n",
    "        num_batches=num_batches,\n",
    "        starting_batch=num_batches*pred_len,\n",
    "        multi_pred_len=True,\n",
    "        retrain=True if pred_len > 0 else False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
