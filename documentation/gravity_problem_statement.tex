

\documentclass[11pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}  % needed for sub figures
\usepackage{enumerate}
\usepackage{paralist}
\usepackage{xcolor} 
\usepackage[hypcap]{caption}
\usepackage[pdftex, colorlinks=true, linkcolor=blue]{hyperref}		% comment in final draft
\usepackage[sort&compress,numbers]{natbib}

% =============================================================
% REDEFINE SECTION
\usepackage{titlesec}

\titleformat{\section}
  [hang]
  {\normalfont\normalsize\bfseries} % format
  {\thesection} % label
  {1em} % horizontal separation between label and title body
  {} % before-code
  [] % after-code
\titlespacing*{\section}  % spacing
  {0em}  % left margin
  {*2} % before separation
  {*2} % after separation

\titleformat{\subsection}
  [runin] % shape
  {\normalfont\normalsize\bfseries} % format
  {\thesubsection} %label
  {0.5em} %horizontal separation between label and title body
  {} % before-code
  [] % after-code
\titlespacing*{\subsection} % spacing
  {\parindent} % left margin
  {*1.5} % before separation
  {*1.3} % after separation
  
\titleformat{\subsubsection}
  [runin] % shape
  {\normalfont\normalsize\bfseries} % format
  {\thesubsubsection} %label
  {0.5em} %horizontal separation between label and title body
  {} % before-code
  [] % after-code
\titlespacing*{\subsubsection} % spacing
  {\parindent} % left margin
  {*1.5} % before separation
  {*1.3} % after separation

\titleformat{\paragraph}
  [runin] % shape
  {\normalfont\normalsize\bfseries} % format
  {} %label
  {0.5em} %horizontal separation between label and title body
  {} % before-code
  [] % after-code
\titlespacing*{\paragraph} % spacing
  {\parindent} % left margin
  {*1.5} % before separation
  {*1.3} % after separation
  
% change section labeling
%\renewcommand\thesection{(\roman{section})}
%\renewcommand\thesubsection{\arabic{subsection}.}

\usepackage{indentfirst}  % indent first paragraph
% =============================================================


\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}  % nice script fonts
\usepackage{mathtools}
\usepackage{bm} % bold math
\usepackage{bbm} % blackboard math \mathbbm

% Quick math font modification
\newcommand{\mc}{\mathcal}
\newcommand{\msf}{\mathsf}
%\newcommand{\mbf}{\bm} 			% requires amsmath, amssymb packages
\newcommand{\mbf}{\mathbf} 			% requires amsmath, amssymb packages
\newcommand{\mbb}{\mathbb}  	% requires bbm
\newcommand{\mf}{\mathfrak}
\newcommand{\mscr}{\mathscr}

% Spaces
\newcommand{\N}{ \ensuremath{\mathbb{N}}}  % natural numbers
\newcommand{\Z}{ \ensuremath{\mathbb{Z}}}  % integers
\newcommand{\Q}{ \ensuremath{\mathbb{Q}}}
\newcommand{\R}{ \ensuremath{\mathbb{R}}}  % real numbers
\newcommand{\C}{ \ensuremath{\mathbb{C}}}  % complex numbers
\newcommand{\K}{ \ensuremath{\mathbb{K}}}  % scalar field \R or \C
\newcommand{\T}{\ensuremath{\mathbb{T}}}
\newcommand{\D}{ \ensuremath{\mathbb{D}}}

% Short-hand greek
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}

% set theory
\providecommand\given{} % make sure it exists
\newcommand\SetSymbol[1][]{
   \nonscript\,#1\vert \allowbreak \nonscript\,\mathopen{}}

\DeclarePairedDelimiterX\set[1]{\lbrace}{\rbrace}{ \renewcommand\given{\SetSymbol[\delimsize]} #1 }  % set* autoscales
\newcommand{\Union}{\bigcup} 		% big union
\newcommand{\union}{\cup} 			% little union
\DeclareMathOperator{\inter}{int}

% Vector spaces/ Operators
\DeclarePairedDelimiterX\norm[1]{\lVert}{\rVert}{#1}  			% norm (\norm*{} autoscales or \norm[\big]{})
\DeclarePairedDelimiterX\inner[2]{\langle}{\rangle}{#1 \,,\, #2}  	% inner product
\newcommand{\transp}{\mathsf{T}}  								% transpose
\DeclareMathOperator{\linspan}{span} 							% linear span
\DeclareMathOperator{\rank}{rank}								% Rank of a matrix.
\DeclareMathOperator{\diag}{diag}								% Diagonal matrix.
\newcommand{\Null}{N}  											% Nullspace
\newcommand{\Ran}{Ran}											% Range
\DeclareMathOperator{\image}{Im}								% Image
\newcommand{\closure}[1]{\overline{#1}}							% Closure

% math functions
\DeclarePairedDelimiterX\abs[1]{\lvert}{\rvert}{#1} % absolute value: \abs{} = no-resize, \abs*{} = left/right auto-resize, \abs[size-cmd]{} = size manually adjusted with size-cmd = \big,\Big,\bigg,\Bigg
\DeclareMathOperator{\sign}{sgn}
\newcommand{\one}{ \ensuremath{\bm{1}}}
\DeclareMathOperator{\supp}{supp}								% Support
%\newcommand{\ind}[1]{\mathbb{I}_{#1}}							% indicator function
\newcommand{\ind}[1]{\mathbbm{1}_{#1}}							% indicator function
\newcommand{\ud}{\,d}  % differential with space before symbol

% Theorem environments
\theoremstyle{plain}
  \newtheorem{theorem}{Theorem}[section]
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{proposition}[theorem]{Proposition}
  \newtheorem{conjecture}[theorem]{Conjecture}
  \newtheorem{claim}[theorem]{Claim}
  \newtheorem{myquestion}[theorem]{Question}
  \newtheorem*{corollary}{Corollary}
\theoremstyle{remark}
  \newtheorem*{remark}{Remark}
  \newtheorem*{remarks}{Remarks}
  \newtheorem*{justification}{Justification}
\theoremstyle{definition}
  \newtheorem{example}[theorem]{Example}
  \newtheorem{definition}[theorem]{Definition}
  \newtheorem*{notation}{Notation}

% Fonts
\usepackage[T1]{fontenc}
%\usepackage{mathptmx}
%\usepackage{fourier}
%\usepackage{tgtermes}
%\usepackage{cmbright}
%\linespread{1.1}

% annotate
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\needcite}{{\tiny\blue{need citation}}}
\newcommand{\todo}[1]{{\tiny\red{#1}}}

% paper-specific definitions
\newcommand{\mat}[1]{\mbf{#1}}  % font style for matrices
\newcommand{\op}[1]{\mathcal{#1}}  % font style for operators
\newcommand{\vsp}[1]{\mathsf{#1}}  % font style for vector spaces
\newcommand{\size}[1]{\mathrm{size}(#1)}
\newcommand{\up}{\uparrow}
\newcommand{\dn}{\downarrow}

\newcommand{\Ko}{C}  % Koopman operator
\newcommand{\Kdm}{A}  % Koopman diffusion operator (asymmetric kernel)
\newcommand{\Dm}{D}  % Diffusion operator (symmetric kernel)
\newcommand{\psdm}{p_{\sigma,\delta}^{(m)}}

\usepackage{lipsum}

\renewcommand{\r}{\bm{p}}
\newcommand{\units}[1]{\mathsf{#1}}


\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}} % short hand bracket matrix
\newcommand{\pmat}[1]{\begin{pmatrix} #1 \end{pmatrix}} % short hand bracket matrix

\newcommand{\rmm}[1]{#1}

%========================================================================
\begin{document}

\noindent\textbf{Contract:} HR0011-18-C-0100

\noindent\textbf{Contractor:} AIMdyn, Inc., 1919 State Street, Suite 207, Santa Barbara, CA 93101

\noindent\textbf{\textsc{R\&D Status Report}}


% == WORK STARTED
\section{The date work started.}



% == Progress description
\section{Description of progress during the reporting period, supported by reasons for any change in approach reported previously.}

% ---- REPORT 3

\subsection{Discovering the energy eigenfunction using LSTM's}
Over the month of August,
we continued coding, implementing, and debugging the associated software for 
for using Deep Learning to find Koopman principal eigenfunctions.
Specifically, we continued developing the software for recurrent networks, 
specifically LSTMs (long-term short-term memory networks). 

We obtained initial results of optimizing 

\begin{equation}
\min_{\phi} \sum_{t=1}^T \dot{\mbf r}_t \cdot \nabla  \phi(\mbf{r}_t),
\end{equation}
where $\phi(\mbf{r}_t)$ is the output of a deep networks with
corresponding inputs $\mbf{r}_t = (x_t,y_t,\dot{x}_t,\dot{y}_t)$, over $T$ time-steps.
The optimization would drive the objective function to a large negative value. However, we did not see that the model would recover an energy function. The corresponding output of the neural net was a square wave that was positive on the first half of the orbit and negative on the second half of the orbit. The energy function (an eigenfunction corresponding to $\lambda = 0$) should be constant on trajectories.

Instead, the following optimization problem was formulated:
\begin{equation}
\min_{\phi} \sum_{t=1}^T \norm{ \dot{\mbf r}_t \cdot \nabla  \phi(\mbf{r}_t) }_2^2 ,
\end{equation}
Hence looking at the sum of $L_2$ norms. This optimization problem forces the output function $\phi$ to satisfy the Koopman eigenfunction criterion
	%% ------------ EQUATION ------------ %%
	\begin{equation}
	\mbf F(\mbf r) \cdot \nabla \phi(\mbf r) = \lambda \phi(\mbf r) = 0 \qquad \text{(since $\lambda = 0$),}
	\end{equation}
	%% ---------------- END  ---------------- %%
at every instance in time. The network was able to learn a 
plausible energy function $\phi(\mbf r)$, which is just a constant. Different constant values of the energy would correspond to different 
initializations of a deep network. 

We are currently looking into ways of constraining the output of a deep network,
so that we could discover nonhomogeneous invariants. But the first result does demonstrate that we are able to discover a meaningful solution. 

\subsection{Learning PDEs from data}
The effort this period had two directions. The first had to do with the use of neural networks in learning partial differential equations; first, partial differential equations for which we ``have explicit equations'', and then, more importantly,
partial differential equations that are ``emergent'' -- that is, equations that arise from, say, coupled oscillator models, and for which we do not really yet have explicit expressions. The second direction had to do with the collaboration with AIMDyN, towards the machine-learning study of planetary data for the ``automated discovery'' of Newtonian gravity. 

\subsection{Diffusion Maps/Data Mining for physical laws}
We have worked on extract physical laws from time series data using Diffusion Maps fro mine the data. Two approaches are being pursued:
\begin{compactenum}[(a)]
\item We are given a time-series of data pairs $\set{ (\mbf r_t, \mbf F_t) : t=0,\dots, T}$, where $\mbf r_t$ is the state at time $t$ and $\mbf F_t$ is the force (right hand side of the ODE) at time $t$. Note $\mbf F_t$ would not be normally given and would have to be determined via automatic differentiation. Using $\mbf F$ directly will allow us to separate algorithmic issues coming from the data mining with those coming from automatic differentiation methods.
\item We are only given state information $\set{ \mbf r_t : t=0,\dots, T}$.
\end{compactenum}

\paragraph{Approach (a):} Consider data from a single planet. If we do Diffusion Maps for the PAIRS of data $(\mbf r_t, \mbf F_t)_{t=0}^{T}$ we will discover ``immediately'' that the data are one-dimensional, with some coordinate $\theta$. This will be some ``graph'' curve in $(\mbf r, \mbf F)$-space. We will be able to write the state and force as functions of the diffusion coordinate $\theta$:
	%% ------------ EQUATION ------------ %%
	\begin{equation}
	\mbf r(\theta) \text{ and } \mbf F(\theta).
	\end{equation}
	%% ---------------- END  ---------------- %%

We will compare obtaining the law if the following ways:
\begin{compactenum}[1.]
\item Train a neural net with $\mbf r$ as input and $\mbf F$ as output.
\item Do semi-supervied learning. For a new point $\hat{\mbf r}$, use the nearby pairs $(\mbf r(\theta),\mbf F(\theta))$, where ``nearby'' is measured using the diffusion coordinate $\theta$, and get the force $\hat{\mbf F}$.
\item Using a set of basis functions to fit the ``graph'' of the data.
\end{compactenum}

A note pertaining to (iii). Given a physical law, there are an infinite number of representations (change of variables) that describe the same physics. The familiar inverse-square law is the most parsimonious. The choice of basis in (iii) used to fit the data determines the parsimony of the representation. The question of parsimony drives a particular choice of basis and will, at this stage, be determined by human intervention/intuition.

If we allow for trajectories from multiple planets as the training set, diffusion maps will find a two-dimensional surface. The first direction will be given by the previously determined phase coordinate $\theta$. The second principal diffusion coordinate (one NOT given by a harmonic of the diffusion eigenfunction corresponding to $\theta$) should correspond to a function dependent on the masses of the planets. We conjecture that these diffusion coordinates will be factorable, in the sense that one varies on the level sets of the other. This will be tested. To get the law, we can train a neural net having two inputs, say $\mbf r$ and $m$, and one output $\mbf F$.

\paragraph{Approach (b):}
This situation by-passes the need for the force or any issues with automatic differentiation. We are given just the state state $\set{\mbf r_t : t=0,\dots, T}$. Many approach learn the time 1 map between the states $\Phi^1(\mbf r_t) = r_{t+1}$. This map, however, has different asymptotic properties than the continuous function it is trying to approximate. This approach can be termed ``\emph{a discrete approximation of a continuous object}''. Our approach can be considered the dual of the previous one: \emph{we learn a continuous function whose discretization gives back the data}. Of course, there is freedom in the choice of discretization. We approach this using neural networks templated off a numerical integration scheme: say Runge-Kutta 4(5): letting $t_i = t_0 + i h$, the RK45 for solving the IVP
	%% ------------ EQUATION ------------ %%
	\begin{equation}
	\begin{aligned}
	y' &= f(t,y) \\
	y(t_0) &= \alpha
	\end{aligned}
	\end{equation}
	%% ---------------- END  ---------------- %%
 is
	%% ------------ EQUATION ------------ %%
	\begin{equation}
	\begin{aligned}
	Y_0 &= \alpha \\
	k_1 &= h\, f(t_i, Y_i) \\
	k_2 &= h\, f(t_i + \frac{h}{2}, Y_i + \frac{k_1}{2}) \\
	k_3 &= h\, f(t_i + \frac{h}{2}, Y_i + \frac{k_2}{2}) \\
	k_4 &= h\, f(t_i + h, Y_i + k_3) \\
	Y_{i+1} &= Y_i + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)
	\end{aligned}
	\end{equation}
	%% ---------------- END  ---------------- %%
We use a deep net to approximate $f$ and take \emph{copies} of it and connect them into the Runge-Kutta scheme. The deep net learns the continuous $f$ and its Runge-Kutta discretization returns the data $\set{ \mbf r_t }_{t=0}^{T}$. Figure \ref{fig:rk45-dnn} gives a schematic of this approach.




% == Planned activities
\section{Planned activities and milestones for the next reporting period.}

\begin{compactenum}
\item Continue exploring constraining the network so that we could discover nonhomogeneous invariants using deep learning.

\item Begin numerical testing of approaches (a) and (b) to ``discover'' Newtonian gravity through data mining. we will use neural nets, diffusion maps and semi supervised learning, and we hope to 

\item Begin exploring how things ``start to fail'' when we approach limits where general relativity becomes important.


\end{compactenum}



% == Major experimental/special equipment purchases
\section{Description of any major items of experimental or special equipment purchased or constructed during the reporting period.}

N/A

% == Key personnel changes
\section{Notification of any changes in key personnel associated with the contract during the reporting period.}

N/A

% == Summary info from trips
\section{Summary of substantive information derived from noteworthy trips, meetings, and special conferences held in connection with the contract during the reporting period.}

N/A

% == Areas of concern
\section{Summary of all problems or areas of concern.}

N/A

% == Related accomplishments since last report
\section{Related accomplishments since last report.}

We have been able to show that we can ``learn'' partial differential equations like the viscous Burgers from data; and that we can use the laws learned (through deep nets) to successfully simulate these PDEs. 



\bibliographystyle{plain}
\bibliography{}

\end{document}

