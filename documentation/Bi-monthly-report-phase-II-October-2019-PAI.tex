\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
%\usepackage{subcaption}  % needed for sub figures
\usepackage{enumerate}
\usepackage{paralist}
\usepackage{xcolor} 
\usepackage[hypcap]{caption}
%\usepackage[pdftex, colorlinks=true, linkcolor=blue]{hyperref}		% comment in final draft
\usepackage[colorlinks=false,urlbordercolor=white]{hyperref}

\usepackage[sort&compress,numbers]{natbib}
\usepackage{wrapfig}
\usepackage{subfig}


% Fonts
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage{mathptmx}
%\usepackage{fourier}
%\usepackage{tgtermes}
%\usepackage{cmbright}
%\linespread{1.1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Formatting for DARPA report
%\usepackage{titlesec}
%\titleformat{\section}[runin]{\normalfont\normalsize\bfseries}{\thesection.}{1em}{}

%% indenting
%\usepackage{indentfirst}

\usepackage{titlesec}
\titleformat*{\section}{\normalsize\bfseries}
%\titleformat*{\subsection}{\normalsize\bfseries}
%\titleformat*{\subsubsection}{\normalsize\bfseries}

\titleformat{\subsection}
{\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}
\titlespacing*{\subsection}{\parindent}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\titleformat{\subsubsection}[runin]
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}
\titlespacing*{\subsubsection}{\parindent}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


%\newcounter{DarpaSecCounter}
%\setcounter{DarpaSecCounter}{0}
%%\newcommand{\rsection}[1]{\stepcounter{DarpaSecCounter}\vspace{15pt}\noindent\textbf{{\theDarpaSecCounter.} #1}\vspace{3pt}}
%\renewcommand{\section}[1]{\stepcounter{DarpaSecCounter}\vspace{15pt}\noindent\textbf{{\theDarpaSecCounter.} #1}\vspace{3pt}}
%
%
%\newcounter{DarpaSubSecCounter}[DarpaSecCounter]
%%\setcounter{DarpaSubSecCounter}{1}
%%\newcommand{\rsubsection}[1]{\stepcounter{DarpaSubSecCounter}\indent\textbf{\theDarpaSubSecCounter. #1}}
%\renewcommand{\subsection}[1]{\vspace{4pt}\stepcounter{DarpaSubSecCounter}\indent\textbf{\theDarpaSecCounter.\theDarpaSubSecCounter. #1.}}
%
%\newcounter{DarpaSubSubSecCounter}[DarpaSubSecCounter]
%\renewcommand{\subsubsection}[1]{\vspace{2pt}\stepcounter{DarpaSubSubSecCounter}\indent\textbf{\theDarpaSecCounter.\theDarpaSubSecCounter.\theDarpaSubSubSecCounter.} \emph{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{mathrsfs}  % nice script fonts
\usepackage{mathtools}
\usepackage{bm} % bold math
%\usepackage{bbm} % blackboard math \mathbbm
%\usepackage{stix}



% Quick math font modification
\newcommand{\mc}{\mathcal}
\newcommand{\msf}{\mathsf}
%\newcommand{\mbf}{\bm} 			% requires amsmath, amssymb packages
\newcommand{\mbf}{\mathbf} 			% requires amsmath, amssymb packages
\newcommand{\mbb}{\mathbb}  	% requires bbm
\newcommand{\mf}{\mathfrak}
\newcommand{\mscr}{\mathscr}

% Spaces
\newcommand{\N}{ \ensuremath{\mathbb{N}}}  % natural numbers
\newcommand{\Z}{ \ensuremath{\mathbb{Z}}}  % integers
\newcommand{\Q}{ \ensuremath{\mathbb{Q}}}
\newcommand{\R}{ \ensuremath{\mathbb{R}}}  % real numbers
\newcommand{\C}{ \ensuremath{\mathbb{C}}}  % complex numbers
\newcommand{\K}{ \ensuremath{\mathbb{K}}}  % scalar field \R or \C
\newcommand{\T}{\ensuremath{\mathbb{T}}}
\newcommand{\D}{ \ensuremath{\mathbb{D}}}

% Short-hand greek
\newcommand{\eps}{\epsilon}
\newcommand{\del}{\delta}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}

% set theory
\providecommand\given{} % make sure it exists
\newcommand\SetSymbol[1][]{
   \nonscript\,#1\vert \allowbreak \nonscript\,\mathopen{}}

\DeclarePairedDelimiterX\set[1]{\lbrace}{\rbrace}{ \renewcommand\given{\SetSymbol[\delimsize]} #1 }  % set* autoscales
\newcommand{\Union}{\bigcup} 		% big union
\newcommand{\union}{\cup} 			% little union
\DeclareMathOperator{\inter}{int}

% Vector spaces/ Operators
\DeclarePairedDelimiterX\norm[1]{\lVert}{\rVert}{#1}  			% norm (\norm*{} autoscales or \norm[\big]{})
\DeclarePairedDelimiterX\inner[2]{\langle}{\rangle}{#1 \,,\, #2}  	% inner product
\newcommand{\transp}{\mathsf{T}}  								% transpose
\DeclareMathOperator{\linspan}{span} 							% linear span
\DeclareMathOperator{\rank}{rank}								% Rank of a matrix.
\DeclareMathOperator{\diag}{diag}								% Diagonal matrix.
\newcommand{\Null}{N}  											% Nullspace
\newcommand{\Ran}{Ran}											% Range
\DeclareMathOperator{\image}{Im}								% Image
%\newcommand{\closure}[1]{\overline{#1}}							% Closure

% math functions
\DeclarePairedDelimiterX\abs[1]{\lvert}{\rvert}{#1} % absolute value: \abs{} = no-resize, \abs*{} = left/right auto-resize, \abs[size-cmd]{} = size manually adjusted with size-cmd = \big,\Big,\bigg,\Bigg
\DeclareMathOperator{\sign}{sgn}
\newcommand{\one}{ \ensuremath{\bm{1}}}
\DeclareMathOperator{\supp}{supp}								% Support
%\newcommand{\ind}[1]{\mathbb{I}_{#1}}							% indicator function
\newcommand{\ind}[1]{\mathbbm{1}_{#1}}							% indicator function
\newcommand{\ud}{\,d}  % differential with space before symbol

% Theorem environments
\theoremstyle{plain}
  \newtheorem{theorem}{Theorem}[section]
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{proposition}[theorem]{Proposition}
  \newtheorem{conjecture}[theorem]{Conjecture}
  \newtheorem{claim}[theorem]{Claim}
  \newtheorem{myquestion}[theorem]{Question}
  \newtheorem*{corollary}{Corollary}
\theoremstyle{remark}
  \newtheorem*{remark}{Remark}
  \newtheorem*{remarks}{Remarks}
  \newtheorem*{justification}{Justification}
\theoremstyle{definition}
  \newtheorem{example}[theorem]{Example}
  \newtheorem{definition}[theorem]{Definition}
  \newtheorem*{notation}{Notation}


% annotate
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\needcite}{{\tiny\blue{need citation}}}
\newcommand{\todo}[1]{{\tiny\red{#1}}}

% paper-specific definitions
\newcommand{\mat}[1]{\mbf{#1}}  % font style for matrices
\newcommand{\op}[1]{\mathcal{#1}}  % font style for operators
\newcommand{\vsp}[1]{\mathsf{#1}}  % font style for vector spaces
\newcommand{\size}[1]{\mathrm{size}(#1)}
\newcommand{\up}{\uparrow}
\newcommand{\dn}{\downarrow}

\newcommand{\Ko}{C}  % Koopman operator
\newcommand{\Kdm}{A}  % Koopman diffusion operator (asymmetric kernel)
\newcommand{\Dm}{D}  % Diffusion operator (symmetric kernel)
\newcommand{\psdm}{p_{\sigma,\delta}^{(m)}}

\usepackage{lipsum}

\renewcommand{\r}{\bm{p}}
\newcommand{\units}[1]{\mathsf{#1}}


\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}} % short hand bracket matrix
\newcommand{\pmat}[1]{\begin{pmatrix} #1 \end{pmatrix}} % short hand bracket matrix

\newcommand{\rmm}[1]{{\color{red}#1}}

\renewcommand{\vec}[1]{\mathbf{#1}}

\newcommand{\x}{\mathbf{x}}

\newcommand{\email}[1]{\href{mailto:#1}{\color{blue}\underline{#1}}}

% MHG packages
\usepackage[version=3]{mhchem}
\usepackage{braket}
\usepackage{relsize}
%\usepackage{enumitem}
%\usepackage[vlined]{algorithm2e}
%\usepackage{algpseudocode}
%\usepackage[font=small,labelfont=bf]{caption}
%\usepackage{graphicx}
%\usepackage{footnote}
%\usepackage{multirow}
%\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{threeparttable}
\usepackage{chemfig}
\usepackage{siunitx}

%========================================================================
\begin{document}

%========================================================================

% == Accomplishments during reporting period

\section{Executive summary}



\begin{figure}

	\centering
	\includegraphics[width=2.5in]{pde_arch_rk4}
	\caption{Illustration of the previous forward Euler architecture. The encoder $\phi(x)$ (shown in green) maps the input into the feature space. Three sequential convolutional layers (shown in orange) are used to approximate $\omega(u_n)$ and summed with $u_n$ (shown in yellow) to give $u_{n+1}$ as outlined in equation \ref{eqn:ode}. At each step, the feature vector $u_n$ is decoded by $\theta(u_n)$ (as shown in light blue) predicting $x_n$. }
	\label{fig:arch_rk4}

\end{figure}


Previously, we developed a forward Euler inspired network architecture that rivaled the performance of the previous LSTM based architectures. This architecture is attractive given its similarity to numeric solvers for partially differential equations which represent a powerful and widely applicable class of simulations that are expensive to compute.

To evaluate new models in a standard way, we first establish a baseline performance using a fully connected linear network, and introduce a relative baseline performance metric 
$$RBP =  \frac{1}{n}\sum_{t=1}^{n}\frac{\sigma_{baseline} - \sigma_{model}}{ \sigma_{baseline}}$$ 

This gives us a score of 1 for models that perfectly explain the test data, and a score of 0 for models as good as a linear model (negative scores perform worse than linear).

In this period we significantly increased the performance of model architectures discovering new learning procedures and model architectures. As compared to our previous LSTM models with relative baseline performance (RBP) of 0.358, further improvements to training procedures (hypothesis 2) increased the RBP of the previously discovered forward Euler architecture to 0.386. 
Furthermore, by introducing multi-scale networks, we see a 28.7\% improvement in mean squared error over 20 time steps resulting in a new architecture with an RBP of 0.548. Finally, we introduce a curriculum learning approach (hypothesis 3) that leverages different scales to train architectures that were previously impossible to train given their sensitivity to co-variate shifts. 


\subsection{Hypothesis 1 - Multi-scale embedded dynamics}

We proposed using convolutions at varying scales to increase the fidelity of the dynamics model while maintaining the symmetry-constrained properties of a convolutional architecture. We evaluate the performance of the forward-Euler architecture with and without multi-scale features and see significant improvement in relative performance resulting in a $28.7\%$ decrease in mean squared error as compared to the previous best architecture.

\subsubsection{What was to be tested? What was the expected outcome prior to testing?}
	We test the addition of convolutional features at different scales to learn the problem dynamics more accurately. Previous architectures used only a single scale for convolutional features meaning the network had access to only local information. By adding multiple scales to the dynamics update we hope to enable the network to learn to use both local and global features in the region to predict more accurately.


\subsubsection{High-level summary of main results.}
	We see strong performance gains by the addition of the additional scales. Predicted sequences are both more accurate overall, and diverge more slowly in initial short-term time-steps.  


\subsubsection{Discuss relevance to project and DARPA concerns.}
	In numeric PDE solvers, choosing the proper simulation scale can dramatically effect the accuracy of simulations even for known PDEs. Given the increase in performance we see that with the addition of multi-scale convolutional layers, networks learn the proper scale for accurate prediction dynamically. This results in networks that are general and adapt well to new problems.


\subsection{Hypothesis 2 - Iterative approximation methods}

 We implemented the fourth-order Runge-Kutta method by iteratively sampling the dynamics model (shown in orange in figure \ref{fig:arch_rk4}) to obtain a more accurate estimate of the next time-step, $u_t$. In testing this method directly on single-scale architectures, performance of the model both computationally and numerically were reduced. Further investigation is needed to determine if a different formulation of the loss function could be effective in lowering compounding error and enabling better long-term predictions. 

\subsubsection{What was to be tested? What was the expected outcome prior to testing?}
	Given the similarity between our network architecture and the update step used by forward-Euler approximations, we proposed integrating techniques effective in reducing the compounding error approximate solutions for ordinary differential equations. Using the existing architecture to predict the next point, taking a small gradient step in that direction, then updating our prediction. We expected this method to increase the accurarcy of the learned model by 


\subsubsection{High-level summary of main results.}
	The fourth order Runge-Kutta method we tested did not improve performance and increased the computational requirements during training. Future is needed to explore if this method could be introduced into the curriculum learning approach outlined in hypothesis 3, as Runge-Kutta as applied in these experiments reduces to an averaging of an iterative quantity which may be intractable given the non-linear nature of the model architecture.


\subsubsection{Discuss relevance to project and DARPA concerns.}
	If successfully applied, iterative methods can leverage traditional understanding of time-discretized ordinary differential equations to improve the accuracy of models without requiring additional training steps or more training data.


\subsection{Hypothesis 3 - Scale adapted curriculum learning}

In experiments concerning hypothesis 2, we experienced an explosion of gradient updates - where small shifts in network parameters were amplified - making learning impossible for these interesting network formulations. To address this issue, we proposed a dynamic adjustment of the prediction horizon while training, forming a curriculum-learning approach where simpler short-term dynamics are learned before introducing longer-term dynamics which have higher sensitivity to rapid shifts in network dynamics. This approach was effective in enabling experiments using these iterative discretize approximation be  


\subsubsection{What was to be tested? What was the expected outcome prior to testing?}
	We evaluate models over increasingly distant time-steps to determine if this procedure regularized the network and prevented explosion of gradients. Given the short reward horizon we expected this method to decrease the variability of gradient updates and allow for stable gradient decent.


\subsubsection{High-level summary of main results.}
This approach expanded the domain of model architectures to experiment on and increased the stability of training allowing for more aggressive gradient steps as well as reducing the amount of data required for early training steps.

\subsubsection{Discuss relevance to project and DARPA concerns.}
By varying the prediction length early on, the decrease in computation dramatically speeds up training time and increases the stability of the learning procedure.
%========================================================================
\section{Achievements}

\subsection{Scientific Breakthroughs}

None

\subsection{Technology developments}

None

\subsection{Application results}

Predictive performance has demonstrated that despite the difficult problem of predicting chaotic systems, using scale-adapted and symmetry constrained architectural components are able to predict better than LSTM based architectures and significantly better then baseline linear models.

\subsection{Transitions achieved}

None

%========================================================================
\section{Lessons Learned}


\noindent Problems encountered/risks that occurred, and corresponding solutions/mitigations

\noindent Open Issues

%========================================================================
\section{Next Steps}

\noindent We would like to further refine the multi-scale architecture, specifically testing networks that include additional scales of convolutional features.


\noindent Current results reflect only one ground truth scale. Testing with super-sampling and sub-sampling to establish scale invariance would be an interesting exploration for the current model architecture.

\noindent When used naively, fourth order Runge-Kutta did not improve the performance of the model. Instead of using Runke-Kutta directly on the same $f(u_t)$ we would like to explore learning separate iterative midpoints to use in the dynamics update step.  


%========================================================================
\section{Technical details}

\begin{figure}
	\begin{minipage}[c]{0.49\textwidth}
		\centering
		\includegraphics[width=2.5in]{pde_arch}
		\caption{Illustration of the previous forward Euler architecture. The encoder $\phi(x)$ (shown in green) maps the input into the feature space. Three convolutional layers (shown in orange) are used to approximate $f(u_n)$ and summed with $u_n$ (shown in yellow) to give $u_{n+1}$ as outlined in equation \ref{eqn:ode}. At each step, the feature vector $u_n$ is decoded by $\theta(u_n)$ (as shown in light blue) predicting $x_n$.}
		\label{fig:arch}

	\end{minipage}
	\begin{minipage}[c]{0.49\textwidth}
		\centering
		\includegraphics[width=2.5in]{pde_arch_multi}
		\caption{Illustration of the multi-scale forward Euler architecture. The
		dynamics $f(u_n)$ are enriched with filters at different scales (shown in orange). Additionally, we reduce the dimensionality of the input image by a factor of 4 in the encoding step $\phi(x)$ to increase the receptive field of the dynamics update calculation, $f(u_n)$. }
		\label{fig:arch}
	\end{minipage}
\end{figure}



\subsection{Formulating Multi-Scale Architecture}

Consider learning a PDE of the form $u_t = f(u)$. The simplest approximation of $u_t = f(u)$ is to discretize the time derivative $u_t$ by $\frac{u_{n+1} - u_n}{\bigtriangleup t}$ and approximate the right hand side by $f(u_n)$. This leads to the forward Euler scheme  

$$u_{n+1} = u_n + \bigtriangleup t f(u_n) \label{eqn:ode} $$

where $f(u_n)$ is learned by a deep neural network. Instead of learning the dynamics of the system directly, we use a learned feature representation $u_n$ composed of linear convolutions of the input history. This avoids the need for approximating the local partial derivatives of the system and instead allows the network to learn the optimal feature representation.


Let $x$ represent h patches of history $x = [x_{t-h}, ..., x_{t-1}]$ and y represent the target sequence $y = 
[x_{t}, x_{t+1},...,x_{t+l-1}]$.  We learn an encoder, $\phi(x) = u_0$, a decoder $\theta(u_n) = \hat{x}_n$ as well as a dynamics model $f(u_n)$ from equation \ref{eqn:ode}.  

In the new multi-scale architecture we utilize a feature space of dimension m = 64, therefore $u_n \in \R^{64, 25, 25}$. Additionally, to ensure spatial continuity, we expand the domain of  $f(u_n)$ to include the local 5x5 region centered about each pixel of a given patch, thus $f(u_n) \in \R^{16 \times 5 \times 5}$ $\rightarrow \R^{16}$. 
Similarity for the encoder, we introduce local features by convolving 16 filters of shape 5x5x5 giving us $\phi(x) \in \R^{5*5*5} \rightarrow \R^{16}$. Finally, for the decoder $\theta(u_n)$ we simply apply a reduction over the feature space, giving  $\theta(u_n) \in \R^{16} \rightarrow \R$

% Thus we learn a series of features $w_{enc}$ such that $u_0 = relu(x * w_{enc})$.
% We approximate the dynamics $f(u)$ by    $f(u_n)$ by

% us to learn the dynamics of a system by approximate the dynamics of these features in the system by $f(u_n)$ by 


\subsection{Evaluating Predictions of Approximated Partial Differential Equations}

\begin{figure}
	\begin{minipage}[t]{0.45\linewidth}
		\centering
		\includegraphics[width=0.7\linewidth]{pde_prediction}
		\caption{Mean absolute error over a single validation batch for PDE architecture. Because of the strong spatially invariant assumptions made by the model, we see strong edge effects near the borders. }
		\label{fig:pdeprediction}
	\end{minipage}
	\begin{minipage}[t]{0.1\linewidth}
		\centering
		\hspace{1\linewidth}
	\end{minipage}
	\begin{minipage}[t]{0.45\linewidth}
		\centering
		\includegraphics[width=0.7\linewidth]{lstm_prediction}
		\caption{Mean absolute error over a single validation batch for LSTM architecture. Note the error is more uniform through the entire patch. }
		\label{fig:lstmprediction}
	\end{minipage}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{comparison_relative}
%	\includegraphics[width=0.9\linewidth]{pde_perf_linear}
	\caption{\small Performance of new PDE architecture for models trained trained using varying sequence lengths. Previous model LSTM model (in red) performs comparably to new model. The high accuracy shown by the short, 5-step prediction experiment suggests that compounding errors from edge effects are likely reducing long term prediction performance.}
	\label{fig:pdeperf}
\end{figure}



As shown in figure \ref{fig:pdeperf}, despite the simplified PDE architecture, the performance of the new models is comparable, being more accurate than the LSTM based model in the near-term samples and slightly less accurate in the long term. As illustrated by figures \ref{fig:pdeprediction} and \ref{fig:lstmprediction} the fully convolutional composition causes edge effects that reduce performance in long horizons as any assumptions made by local features valid in the center of the patch are violated when evaluated near the edges of the patch.


\end{document}
